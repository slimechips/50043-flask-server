{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tfidf.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynOvBCfZ8Hac"
      },
      "source": [
        "import matplotlib.pyplot as plt # plotting library\n",
        "import numpy as np # library for numerical calculations\n",
        "import pandas as pd # library for reading csv files"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlg73iu-8JHh"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import Counter\n",
        "from sklearn.feature_selection import SelectPercentile, chi2, f_regression\n",
        "data1 = pd.read_csv(\"output_all.csv\",nrows=10000)\n",
        "corpus =data1['news']\n",
        "#print(corpus)\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "#print(vectorizer.get_feature_names())\n",
        "\n",
        "z=X.toarray()\n",
        "#term frequency is printed\n",
        "print(z)\n",
        "\n",
        "vectorizer1 = TfidfVectorizer(min_df=1)\n",
        "X1 = vectorizer1.fit_transform(corpus)\n",
        "idf = vectorizer1.idf_\n",
        "#print (dict(zip(vectorizer1.get_feature_names(), idf)))\n",
        "#printing idf\n",
        "#print(X1.toarray())\n",
        "#printing tfidf\n",
        "TFIDF_1=X1.toarray()\n",
        "#TFIDF_2 = TFIDF_1.sum(axis=1)\n",
        "#TFIDF_2 = TFIDF_2.reshape(478,1)\n",
        "#TFIDF_2\n",
        "def judgeLevel(df):\n",
        "    if df['t7Close']-df['tClose']< 0:\n",
        "        return -1\n",
        "    elif df['t7Close']-df['tClose'] == 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "data1['trend']=data1.apply(lambda r:judgeLevel(r),axis=1)\n",
        "#data1['TFIDF']=TFIDF_2\n",
        "tfidf_new = SelectPercentile(f_regression, percentile=1).fit_transform(TFIDF_1, data1['trend'])\n",
        "tfidf_new.shape\n",
        "data1['trend']=data1.apply(lambda r:judgeLevel(r),axis=1)\n",
        "data1\n",
        "tfidf_new.shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAIb7JnC8Ws7"
      },
      "source": [
        "import matplotlib.pyplot as plt # plotting library\n",
        "import numpy as np # library for numerical calculations\n",
        "import pandas as pd # library for reading csv files\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data1 = pd.read_csv('output_all.csv',nrows=10000)\n",
        "#data1.head()\n",
        "#data1.loc[[1],['longitude']]=data1.loc[[1],['latitude']] - data1.loc[[1],['housing_median_age']]\n",
        "#data1.dtypes\n",
        "#data1.head()\n",
        "def judgeLevel(df):\n",
        "    if df['t7Close']-df['tClose']< 0:\n",
        "        return -1\n",
        "    #elif df['t7Close']-df['tClose'] == 0:\n",
        "    #    return 0\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "def evaluate_on_training_set(y_test, y_pred,name):\n",
        "  # Calculate AUC\n",
        "  auc_score = roc_auc_score(y_test,y_pred)\n",
        "  print(\"AUC is: \", auc_score ) #todo print AUC score\n",
        "  # print out recall and precision\n",
        "  print(classification_report(y_test, y_pred))\n",
        "  \n",
        "  # print out confusion matrix\n",
        "  print(\"Confusion Matrix: \\n\", confusion_matrix(y_test, y_pred))\n",
        "\n",
        "  # # calculate points for ROC curve\n",
        "  fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "  \n",
        "  # Plot ROC curve\n",
        "  plt.plot(fpr, tpr, label= name + '(area = %0.3f)' % roc_auc_score(y_test, y_pred) )\n",
        "  plt.plot([0, 1], [0, 1], 'k--')  # random predictions curve\n",
        "  plt.legend(loc='lower right')\n",
        "  plt.xlim([0.0, 1.0])\n",
        "  plt.ylim([0.0, 1.0])\n",
        "  plt.xlabel('False Positive Rate or (1 - Specifity)')\n",
        "  plt.ylabel('True Positive Rate or (Sensitivity)')\n",
        "  plt.title('Receiver Operating Characteristic')\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import Counter\n",
        "\n",
        "corpus =data1['news']\n",
        "print(corpus)\n",
        "\n",
        "data1['trend']=data1.apply(lambda r:judgeLevel(r),axis=1)\n",
        "#data1['TFIDF']=TFIDF_2\n",
        "\n",
        "#X_train, X_test, y_train, y_test = train_test_split(data1['TFIDF'].values.reshape(478,1), \n",
        "#                                                    data1['trend'],\n",
        "#                                                    test_size=0.3,\n",
        "#                                                    random_state=0) \n",
        "X_train, X_test, y_train, y_test = train_test_split(tfidf_new, \n",
        "                                                    data1['trend'],\n",
        "                                                    test_size=0.3,\n",
        "                                                    random_state=0) \n",
        "# define a new scaler: \n",
        "x_scaler = MinMaxScaler()\n",
        "# fit the normalization on the training set: \n",
        "x_scaler.fit(X_train)     #todo\n",
        "\n",
        "# then create new and normalized training/test sets: \n",
        "X_train_norm = x_scaler.transform(X_train)\n",
        "X_test_norm = x_scaler.transform(X_test)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "model = KNeighborsClassifier(n_neighbors=33) # Define the model with parameters\n",
        "\n",
        "model.fit(X_train_norm, y_train) # Training the model\n",
        "\n",
        "# Evaluate the model: \n",
        "y_pred = model.predict(X_test_norm) # Predicting labels for our test set using trained model\n",
        "evaluate_on_training_set(y_test, y_pred,'KNC') #evaluate our model using newly defined function\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "model = GaussianNB() # Define the model with parameters\n",
        "model.fit(X_train_norm, y_train) # Training the model\n",
        "\n",
        "y_pred = model.predict(X_test_norm) # Predicting labels for our test set using trained model\n",
        "evaluate_on_training_set(y_test, y_pred,'GNB') #evaluate our model using newly defined function\n",
        "\n",
        "from sklearn import tree\n",
        "\n",
        "model = tree.DecisionTreeClassifier(max_depth=3, min_samples_leaf=1) \n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test) # Predicting labels for our test set using model\n",
        "print (y_pred)\n",
        "evaluate_on_training_set(y_test, y_pred,'DTC') #evaluate our model using new function\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "model = SVC(C=10, gamma='auto', kernel='rbf')\n",
        "model.fit(X_train_norm, y_train)\n",
        "model.fit(X_train_norm, y_train) # Training SVM\n",
        "\n",
        "y_pred = model.predict(X_test_norm) # Predicting labels for our test set using trained model\n",
        "evaluate_on_training_set(y_test, y_pred,'SVC') #evaluate our model using newly defined \n",
        "\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "model = AdaBoostClassifier(n_estimators=1000, learning_rate=0.1) # Define the model with parameters\n",
        "model.fit(X_train_norm, y_train) # Training the model\n",
        "\n",
        "y_pred = model.predict(X_test_norm) # Predicting labels for our test set using trained model\n",
        "evaluate_on_training_set(y_test, y_pred,'ABC') #evaluate our model using newly defined function\n",
        "\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model = RandomForestClassifier(n_estimators = 50) # Define the model\n",
        "#TODO fit the model, predict y and evaluate as before\n",
        "model.fit(X_train_norm, y_train) # Training the model\n",
        "\n",
        "y_pred = model.predict(X_test_norm) # Predicting labels for our test set using trained model\n",
        "evaluate_on_training_set(y_test, y_pred,'RFC') #evaluate our model using newly defined function"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}